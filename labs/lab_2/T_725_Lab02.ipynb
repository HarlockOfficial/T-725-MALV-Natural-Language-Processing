{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pajD7caf9pX7"
   },
   "source": [
    "# T-725 Natural Language Processing: Lab 2\n",
    "In today's lab, we will be working with text classification.\n",
    "\n",
    "To begin with, do the following:\n",
    "* Select `\"File\" > \"Save a copy in Drive\"` to create a local copy of this notebook that you can edit.\n",
    "* Select `\"Runtime\" > \"Run all\"` to run the code in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RsSxs1XKhi_4"
   },
   "source": [
    "## List comprehensions in Python\n",
    "List comprehensions are a concise way of creating lists in Python, and take the form:\n",
    "\n",
    "```python\n",
    "[expression for item in iterable]\n",
    "```\n",
    "\n",
    "A list comprehension creates a new list by evaluating some expression for every item in a given iterable (such as a string, a list or a dictionary). Let's look at an example:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "wxB1Ip77hzFL",
    "ExecuteTime": {
     "end_time": "2023-09-08T12:55:10.104434032Z",
     "start_time": "2023-09-08T12:55:10.025510262Z"
    }
   },
   "source": [
    "sentence = \"In a hole in the ground there lived a hobbit.\"\n",
    "words = sentence.split()\n",
    "print(words)\n",
    "\n",
    "# Example of a list comprehension\n",
    "word_lengths = [len(word) for word in words]\n",
    "print(word_lengths)\n",
    "\n",
    "# This is equal to\n",
    "word_lengths = []\n",
    "for word in words:\n",
    "  word_lengths.append(len(word))\n",
    "\n",
    "print(word_lengths)"
   ],
   "execution_count": 142,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In', 'a', 'hole', 'in', 'the', 'ground', 'there', 'lived', 'a', 'hobbit.']\n",
      "[2, 1, 4, 2, 3, 6, 5, 5, 1, 7]\n",
      "[2, 1, 4, 2, 3, 6, 5, 5, 1, 7]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aWw6IRBgkQbc"
   },
   "source": [
    "You can also add a conditional statement to list comprehensions, so that the expression will only be evaluated for items that meet a certain criteria:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "dSRG8kBdjZKA",
    "ExecuteTime": {
     "end_time": "2023-09-08T12:52:41.918770952Z",
     "start_time": "2023-09-08T12:52:40.535470955Z"
    }
   },
   "source": [
    "e_words = [word for word in words if len(word) > 5]\n",
    "print(e_words)"
   ],
   "execution_count": 119,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ground', 'hobbit.']\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qtTTJDJ5qZML"
   },
   "source": [
    "Python also has set and dictionary comprehensions:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "jniqHWPoqd8E",
    "ExecuteTime": {
     "end_time": "2023-09-08T12:52:42.107831352Z",
     "start_time": "2023-09-08T12:52:40.559674674Z"
    }
   },
   "source": [
    "lowercase_characters = {c.lower() for c in sentence}\n",
    "print(lowercase_characters)\n",
    "\n",
    "word_length = {word: len(word) for word in words}\n",
    "print(word_length['ground'])"
   ],
   "execution_count": 120,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'u', 'v', 't', 'g', 'n', 'a', 'r', 'h', 'o', 'l', 'b', 'i', '.', 'e', ' ', 'd'}\n",
      "6\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vY3Y_4kIbpaU"
   },
   "source": [
    "A nested list is a list within another list. You can iterate through nested lists in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "6AW8w46PbuFM",
    "ExecuteTime": {
     "end_time": "2023-09-08T12:52:42.208225004Z",
     "start_time": "2023-09-08T12:52:40.589492246Z"
    }
   },
   "source": [
    "# A list of countries and their capitals within different continents\n",
    "continents = [\n",
    "    [('Iceland', 'Reykjavík'), ('Germany', 'Berlin'), ('Spain', 'Madrid')],  # Europe\n",
    "    [('Japan', 'Tokyo'), ('China', 'Beijing'), ('South Korea', 'Seoul')],  # Asia\n",
    "    [('Nigeria', 'Abuja'), ('Algeria', 'Algiers'), ('Angola', 'Luanda')]  # Africa\n",
    "]\n",
    "\n",
    "# Create a list of all the countries in the previous list\n",
    "[country for continent in continents for (country, capital) in continent]"
   ],
   "execution_count": 121,
   "outputs": [
    {
     "data": {
      "text/plain": "['Iceland',\n 'Germany',\n 'Spain',\n 'Japan',\n 'China',\n 'South Korea',\n 'Nigeria',\n 'Algeria',\n 'Angola']"
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mBrOT1vNcYZ0"
   },
   "source": [
    "## Sentiment analysis with NLTK\n",
    "[Chapter 6](https://www.nltk.org/book/ch06.html) of the NLTK book shows how the toolkit can be used to create document classifiers, including a sentiment analyzer. The NLTK includes the `movie_reviews` corpus, which contains 2,000 movie reviews. Half of the reviews have been labelled as **positive** and the other half as **negative**. Let's download it and take a look:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Xx01nR8x9qgq",
    "ExecuteTime": {
     "end_time": "2023-09-08T12:52:42.284696958Z",
     "start_time": "2023-09-08T12:52:40.634702706Z"
    }
   },
   "source": [
    "import nltk\n",
    "from nltk.corpus import movie_reviews\n",
    "nltk.download('punkt')\n",
    "\n",
    "nltk.download('movie_reviews')\n",
    "print(\"Categories:\", movie_reviews.categories())"
   ],
   "execution_count": 122,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categories: ['neg', 'pos']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/administrator/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     /home/administrator/nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AppRxScjc9vx"
   },
   "source": [
    "As expected, there are two categories: `pos` for positive reviews and `neg` for negative reviews. For this particular corpus, each review is stored as a separate text file. To get a list of all the text files in the corpus, we can use `movie_reviews.fileids()`. We can also get a list of files for a specific category:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "GI8EjJoo_1jz",
    "ExecuteTime": {
     "end_time": "2023-09-08T12:52:42.406523317Z",
     "start_time": "2023-09-08T12:52:40.664014933Z"
    }
   },
   "source": [
    "pos_fileids = movie_reviews.fileids('pos')\n",
    "neg_fileids = movie_reviews.fileids('neg')\n",
    "\n",
    "print(pos_fileids[:5])  # The first 5 positive reviews\n",
    "print(neg_fileids[:5])  # The first 5 negative reviews"
   ],
   "execution_count": 123,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pos/cv000_29590.txt', 'pos/cv001_18431.txt', 'pos/cv002_15918.txt', 'pos/cv003_11664.txt', 'pos/cv004_11636.txt']\n",
      "['neg/cv000_29416.txt', 'neg/cv001_19502.txt', 'neg/cv002_17424.txt', 'neg/cv003_12683.txt', 'neg/cv004_12641.txt']\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OQ_C9_80AEDo"
   },
   "source": [
    "We can get a list of all the tokens in the corpus with `movie_reviews.words()`. We can also specify a filename to get a single tokenized review:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "vZ8mPNPGcnbN",
    "ExecuteTime": {
     "end_time": "2023-09-08T12:52:42.661130388Z",
     "start_time": "2023-09-08T12:52:40.705634036Z"
    }
   },
   "source": [
    "pos_reviews = [movie_reviews.words(fid) for fid in pos_fileids]\n",
    "neg_reviews = [movie_reviews.words(fid) for fid in neg_fileids]\n",
    "\n",
    "print(pos_reviews[0][:10])  # The first 10 tokens of the first positive review\n",
    "print(neg_reviews[0][:10])  # The first 10 tokens of the first negative review"
   ],
   "execution_count": 124,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['films', 'adapted', 'from', 'comic', 'books', 'have', 'had', 'plenty', 'of', 'success']\n",
      "['plot', ':', 'two', 'teen', 'couples', 'go', 'to', 'a', 'church', 'party']\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g04vosHorN6F"
   },
   "source": [
    "Some words, such as *brilliant* and *memorable*, are more strongly associated with positive reviews than negative ones. Similarly, *boring* and *unfunny* have a stronger association with negative reviews.\n",
    "\n",
    "Using the movie review corpus, we can train a classifier to predict whether a given review is positive or negative. The classifier extracts a set of *features* from every review, which are then used to make the classification. In this case, the features we use will be a dictionary that tells us whether each of the 2,000 most common words in the corpus is present within a review or not."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "UeixOJ-lu1fD",
    "ExecuteTime": {
     "end_time": "2023-09-08T12:52:44.488328726Z",
     "start_time": "2023-09-08T12:52:40.835861192Z"
    }
   },
   "source": [
    "# Create a set with 2,000 of the most frequent words in the movie review corpus\n",
    "movie_fd = nltk.FreqDist(movie_reviews.words())\n",
    "movie_words = {word for word, count in movie_fd.most_common(2000)}\n",
    "\n",
    "# For a given review (in the form of a list or set of tokens), create a\n",
    "# dictionary which tells us which words are present and which are not.\n",
    "def get_review_features(review):\n",
    "  review_words = set(review)\n",
    "  return {word: word in review_words for word in movie_words}"
   ],
   "execution_count": 125,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "6-r6kXma8-eB",
    "ExecuteTime": {
     "end_time": "2023-09-08T12:52:44.507186915Z",
     "start_time": "2023-09-08T12:52:43.054952929Z"
    }
   },
   "source": [
    "# Let's see how this works for the first positive review:\n",
    "example_features = get_review_features(pos_reviews[0])\n",
    "print(\"'funny' is in the review:\", example_features['funny'])\n",
    "print(\"'boring' is in the review:\", example_features['boring'])"
   ],
   "execution_count": 126,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'funny' is in the review: True\n",
      "'boring' is in the review: False\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KK5rLkC7TCVC"
   },
   "source": [
    "Next, let's create a training set that we can use to train a Naive Bayesian classifier. The training set, in this case, is a list of tuples in the format `[(features, category), ...]`, where `features` is a dictionary from `get_review_features()` and `category` is either `pos` or `neg`, depending on whether the review is positive or negative. To get an idea of how well the classifier performs, we're going to reserve 10% of the reviews for testing. That means that we'll be training our classifier on 1800 examples and testing it on 200 examples."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "MqhiywiwrhxW",
    "ExecuteTime": {
     "end_time": "2023-09-08T12:52:45.423648213Z",
     "start_time": "2023-09-08T12:52:43.089492735Z"
    }
   },
   "source": [
    "pos_examples = [(get_review_features(review), 'pos') for review in pos_reviews]\n",
    "neg_examples = [(get_review_features(review), 'neg') for review in neg_reviews]\n",
    "\n",
    "movie_training = pos_examples[:900] + neg_examples[:900]  # 1800 examples total\n",
    "movie_test = pos_examples[900:] + neg_examples[900:]  # 200 examples total"
   ],
   "execution_count": 127,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jfty2sigVuf3"
   },
   "source": [
    "Now we have everything we need to train our classifier."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "zAqq1Z72wLHl",
    "ExecuteTime": {
     "end_time": "2023-09-08T12:52:48.620185562Z",
     "start_time": "2023-09-08T12:52:45.420521113Z"
    }
   },
   "source": [
    "movie_classifier = nltk.NaiveBayesClassifier.train(movie_training)"
   ],
   "execution_count": 128,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3inpPaRDV0_6"
   },
   "source": [
    "How well does it perform on the test set?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "y7eJMkqdw7Eb",
    "ExecuteTime": {
     "end_time": "2023-09-08T12:52:49.608516488Z",
     "start_time": "2023-09-08T12:52:48.642112155Z"
    }
   },
   "source": [
    "print(\"Accuracy:\", nltk.classify.accuracy(movie_classifier, movie_test))"
   ],
   "execution_count": 129,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.815\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZxakIFH5WNH_"
   },
   "source": [
    "The classifier achieves an accuracy of 81.5%. Let's take a look at which words have the biggest weights:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ibMPC0QCxefo",
    "ExecuteTime": {
     "end_time": "2023-09-08T12:52:49.741910433Z",
     "start_time": "2023-09-08T12:52:49.609567147Z"
    }
   },
   "source": [
    "movie_classifier.show_most_informative_features(20)"
   ],
   "execution_count": 130,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "             outstanding = True              pos : neg    =     15.6 : 1.0\n",
      "                   mulan = True              pos : neg    =      9.0 : 1.0\n",
      "             wonderfully = True              pos : neg    =      7.1 : 1.0\n",
      "                  seagal = True              neg : pos    =      7.0 : 1.0\n",
      "                   damon = True              pos : neg    =      6.1 : 1.0\n",
      "                   flynt = True              pos : neg    =      5.7 : 1.0\n",
      "                  wasted = True              neg : pos    =      5.6 : 1.0\n",
      "                    lame = True              neg : pos    =      5.3 : 1.0\n",
      "                  poorly = True              neg : pos    =      5.2 : 1.0\n",
      "                   awful = True              neg : pos    =      4.9 : 1.0\n",
      "              ridiculous = True              neg : pos    =      4.8 : 1.0\n",
      "                    jedi = True              pos : neg    =      4.4 : 1.0\n",
      "                 unfunny = True              neg : pos    =      4.4 : 1.0\n",
      "                   waste = True              neg : pos    =      4.4 : 1.0\n",
      "               fantastic = True              pos : neg    =      4.4 : 1.0\n",
      "                   worst = True              neg : pos    =      4.2 : 1.0\n",
      "                    mess = True              neg : pos    =      4.2 : 1.0\n",
      "                  stupid = True              neg : pos    =      4.2 : 1.0\n",
      "                   bland = True              neg : pos    =      4.0 : 1.0\n",
      "                     era = True              pos : neg    =      4.0 : 1.0\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FPSLZUxt9uMY"
   },
   "source": [
    "# Assignment\n",
    "Answer the following questions and hand in your solution in Canvas before 8:30 AM Monday, September 11th. Remember to save your file before uploading it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tQN5Qq9z97Yc"
   },
   "source": [
    "## Question 1\n",
    "The NLTK also includes a `subjectivity` corpus, which contains a collection of sentences that have either been categorized as **subjective** (emotional, expressing personal feelings and views)  or **objective** (more rational, factual). Some examples:\n",
    "\n",
    "* **Objective sentences**:\n",
    "  * uma thurman stars in quentin tarantino's fourth film venture , kill bill .  \n",
    "  * he lives in a motor garage with his six friends .\n",
    "  * the ensuing battle was one of the most savage in u . s . history .\n",
    "* **Subjective sentences**:\n",
    "  * seagal's strenuous attempt at a change in expression could very well clinch him this year's razzie .\n",
    "  * de niro cries . you'll cry for your money back .\n",
    "  * a heroic tale of persistence that is sure to win viewers' hearts .\n",
    "\n",
    "Unlike the movie review corpus, where every review is stored in separate file, here there is only one file for each category.\n",
    "\n",
    "Complete the following tasks:\n",
    "1. Import and download the `subjectivity` corpus.\n",
    "2. Find the names of each category.\n",
    "3. Using the category names, get the relative path of each file.\n",
    "4. Get a list of tokenized sentences for each category (using `subjectivity.sents(fileid)`)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "zc1EM7TvnYaC",
    "ExecuteTime": {
     "end_time": "2023-09-08T12:52:49.940739247Z",
     "start_time": "2023-09-08T12:52:49.668338342Z"
    }
   },
   "source": [
    "import nltk\n",
    "from nltk.corpus import subjectivity\n",
    "nltk.download('subjectivity')\n",
    "\n",
    "category_names = subjectivity.categories()\n",
    "print(\"There are {} categories: {}\".format(len(category_names), category_names))\n",
    "# There are 2 categories: ['obj', 'subj']\n",
    "\n",
    "category_files = [subjectivity.fileids(category) for category in category_names]\n",
    "print(\"There are {} files in the 'obj' category and {} files in the 'subj' category\".format(len(category_files[0]), len(category_files[1])))\n",
    "# There are 1 files in the 'obj' category and 1 files in the 'subj' category\n",
    "\n",
    "category_sentences = [subjectivity.sents(fileid) for fileid in category_files]\n",
    "print(\"There are {} sentences in the 'obj' category and {} sentences in the 'subj' category\".format(len(category_sentences[0]), len(category_sentences[1])))\n",
    "# There are 5000 sentences in the 'obj' category and 5000 sentences in the 'subj' category\n",
    "print(\"Extract of the first 5 sentences in the 'obj' category: {}\".format(category_sentences[0][:5]))\n",
    "'''\n",
    "Extract of the first 5 sentences in the 'obj' category: [['the', 'movie', 'begins', 'in', 'the', 'past', 'where', 'a', 'young', 'boy', 'named', 'sam', 'attempts', 'to', 'save', 'celebi', 'from', 'a', 'hunter', '.'], ['emerging', 'from', 'the', 'human', 'psyche', 'and', 'showing', 'characteristics', 'of', 'abstract', 'expressionism', ',', 'minimalism', 'and', 'russian', 'constructivism', ',', 'graffiti', 'removal', 'has', 'secured', 'its', 'place', 'in', 'the', 'history', 'of', 'modern', 'art', 'while', 'being', 'created', 'by', 'artists', 'who', 'are', 'unconscious', 'of', 'their', 'artistic', 'achievements', '.'], ['spurning', 'her', \"mother's\", 'insistence', 'that', 'she', 'get', 'on', 'with', 'her', 'life', ',', 'mary', 'is', 'thrown', 'out', 'of', 'the', 'house', ',', 'rejected', 'by', 'joe', ',', 'and', 'expelled', 'from', 'school', 'as', 'she', 'grows', 'larger', 'with', 'child', '.'], ['amitabh', \"can't\", 'believe', 'the', 'board', 'of', 'directors', 'and', 'his', 'mind', 'is', 'filled', 'with', 'revenge', 'and', 'what', 'better', 'revenge', 'than', 'robbing', 'the', 'bank', 'himself', ',', 'ironic', 'as', 'it', 'may', 'sound', '.'], ['she', ',', 'among', 'others', 'excentricities', ',', 'talks', 'to', 'a', 'small', 'rock', ',', 'gertrude', ',', 'like', 'if', 'she', 'was', 'alive', '.']]\n",
    "'''\n",
    "print(\"Extract of the first 5 sentences in the 'subj' category: {}\".format(category_sentences[1][:5]))\n",
    "'''\n",
    "Extract of the first 5 sentences in the 'subj' category: [['smart', 'and', 'alert', ',', 'thirteen', 'conversations', 'about', 'one', 'thing', 'is', 'a', 'small', 'gem', '.'], ['color', ',', 'musical', 'bounce', 'and', 'warm', 'seas', 'lapping', 'on', 'island', 'shores', '.', 'and', 'just', 'enough', 'science', 'to', 'send', 'you', 'home', 'thinking', '.'], ['it', 'is', 'not', 'a', 'mass-market', 'entertainment', 'but', 'an', 'uncompromising', 'attempt', 'by', 'one', 'artist', 'to', 'think', 'about', 'another', '.'], ['a', 'light-hearted', 'french', 'film', 'about', 'the', 'spiritual', 'quest', 'of', 'a', 'fashion', 'model', 'seeking', 'peace', 'of', 'mind', 'while', 'in', 'a', 'love', 'affair', 'with', 'a', 'veterinarian', 'who', 'is', 'a', 'non-practicing', 'jew', '.'], ['my', 'wife', 'is', 'an', 'actress', 'has', 'its', 'moments', 'in', 'looking', 'at', 'the', 'comic', 'effects', 'of', 'jealousy', '.', 'in', 'the', 'end', ',', 'though', ',', 'it', 'is', 'only', 'mildly', 'amusing', 'when', 'it', 'could', 'have', 'been', 'so', 'much', 'more', '.']]\n",
    "'''"
   ],
   "execution_count": 131,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2 categories: ['obj', 'subj']\n",
      "There are 1 files in the 'obj' category and 1 files in the 'subj' category\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package subjectivity to\n",
      "[nltk_data]     /home/administrator/nltk_data...\n",
      "[nltk_data]   Package subjectivity is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 5000 sentences in the 'obj' category and 5000 sentences in the 'subj' category\n",
      "Extract of the first 5 sentences in the 'obj' category: [['the', 'movie', 'begins', 'in', 'the', 'past', 'where', 'a', 'young', 'boy', 'named', 'sam', 'attempts', 'to', 'save', 'celebi', 'from', 'a', 'hunter', '.'], ['emerging', 'from', 'the', 'human', 'psyche', 'and', 'showing', 'characteristics', 'of', 'abstract', 'expressionism', ',', 'minimalism', 'and', 'russian', 'constructivism', ',', 'graffiti', 'removal', 'has', 'secured', 'its', 'place', 'in', 'the', 'history', 'of', 'modern', 'art', 'while', 'being', 'created', 'by', 'artists', 'who', 'are', 'unconscious', 'of', 'their', 'artistic', 'achievements', '.'], ['spurning', 'her', \"mother's\", 'insistence', 'that', 'she', 'get', 'on', 'with', 'her', 'life', ',', 'mary', 'is', 'thrown', 'out', 'of', 'the', 'house', ',', 'rejected', 'by', 'joe', ',', 'and', 'expelled', 'from', 'school', 'as', 'she', 'grows', 'larger', 'with', 'child', '.'], ['amitabh', \"can't\", 'believe', 'the', 'board', 'of', 'directors', 'and', 'his', 'mind', 'is', 'filled', 'with', 'revenge', 'and', 'what', 'better', 'revenge', 'than', 'robbing', 'the', 'bank', 'himself', ',', 'ironic', 'as', 'it', 'may', 'sound', '.'], ['she', ',', 'among', 'others', 'excentricities', ',', 'talks', 'to', 'a', 'small', 'rock', ',', 'gertrude', ',', 'like', 'if', 'she', 'was', 'alive', '.']]\n",
      "Extract of the first 5 sentences in the 'subj' category: [['smart', 'and', 'alert', ',', 'thirteen', 'conversations', 'about', 'one', 'thing', 'is', 'a', 'small', 'gem', '.'], ['color', ',', 'musical', 'bounce', 'and', 'warm', 'seas', 'lapping', 'on', 'island', 'shores', '.', 'and', 'just', 'enough', 'science', 'to', 'send', 'you', 'home', 'thinking', '.'], ['it', 'is', 'not', 'a', 'mass-market', 'entertainment', 'but', 'an', 'uncompromising', 'attempt', 'by', 'one', 'artist', 'to', 'think', 'about', 'another', '.'], ['a', 'light-hearted', 'french', 'film', 'about', 'the', 'spiritual', 'quest', 'of', 'a', 'fashion', 'model', 'seeking', 'peace', 'of', 'mind', 'while', 'in', 'a', 'love', 'affair', 'with', 'a', 'veterinarian', 'who', 'is', 'a', 'non-practicing', 'jew', '.'], ['my', 'wife', 'is', 'an', 'actress', 'has', 'its', 'moments', 'in', 'looking', 'at', 'the', 'comic', 'effects', 'of', 'jealousy', '.', 'in', 'the', 'end', ',', 'though', ',', 'it', 'is', 'only', 'mildly', 'amusing', 'when', 'it', 'could', 'have', 'been', 'so', 'much', 'more', '.']]\n"
     ]
    },
    {
     "data": {
      "text/plain": "\"\\nExtract of the first 5 sentences in the 'subj' category: [['smart', 'and', 'alert', ',', 'thirteen', 'conversations', 'about', 'one', 'thing', 'is', 'a', 'small', 'gem', '.'], ['color', ',', 'musical', 'bounce', 'and', 'warm', 'seas', 'lapping', 'on', 'island', 'shores', '.', 'and', 'just', 'enough', 'science', 'to', 'send', 'you', 'home', 'thinking', '.'], ['it', 'is', 'not', 'a', 'mass-market', 'entertainment', 'but', 'an', 'uncompromising', 'attempt', 'by', 'one', 'artist', 'to', 'think', 'about', 'another', '.'], ['a', 'light-hearted', 'french', 'film', 'about', 'the', 'spiritual', 'quest', 'of', 'a', 'fashion', 'model', 'seeking', 'peace', 'of', 'mind', 'while', 'in', 'a', 'love', 'affair', 'with', 'a', 'veterinarian', 'who', 'is', 'a', 'non-practicing', 'jew', '.'], ['my', 'wife', 'is', 'an', 'actress', 'has', 'its', 'moments', 'in', 'looking', 'at', 'the', 'comic', 'effects', 'of', 'jealousy', '.', 'in', 'the', 'end', ',', 'though', ',', 'it', 'is', 'only', 'mildly', 'amusing', 'when', 'it', 'could', 'have', 'been', 'so', 'much', 'more', '.']]\\n\""
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UaXJ-5Lr97_w"
   },
   "source": [
    "## Question 2\n",
    "Complete the following tasks:\n",
    "1. Create a set with the 2,000 most common words in the `subjectivity` corpus using `nltk.FreqDist()`.\n",
    "2. Create a function that takes a single, tokenized sentence as input (e.g., `['the', 'ensuing', 'battle', ...]`), and returns a dictionary of the 2,000 most frequent words and whether or not they are in the sentence (e.g., `{'battle': True, 'amusing': False, ...}`)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "3Bx4kZSW98xq",
    "ExecuteTime": {
     "end_time": "2023-09-08T12:52:50.318889395Z",
     "start_time": "2023-09-08T12:52:49.905741053Z"
    }
   },
   "source": [
    "subjectivity_frequency_distribution = nltk.FreqDist(subjectivity.words())\n",
    "print(\"There are {} unique words in the 'subjectivity' corpus\".format(len(subjectivity_frequency_distribution)))\n",
    "# There are 23906 unique words in the 'subjectivity' corpus\n",
    "\n",
    "top_2000_words = {word for word, count in subjectivity_frequency_distribution.most_common(2000)}\n",
    "print(\"The 2000 most common words are: {}\".format(top_2000_words))\n",
    "# Note: output omitted from the code, but the set contains the 2000 most common words\n",
    "\n",
    "def to_word_occurrence_dictionary(tokenized_sentence, top_words=top_2000_words) -> dict[str, int]:\n",
    "    word_occurrence_dictionary = dict()\n",
    "    for top_word in top_words:\n",
    "        word_occurrence_dictionary[top_word] = len([word for word in tokenized_sentence if word == top_word])\n",
    "    return word_occurrence_dictionary\n",
    "\n",
    "def to_word_presence_dictionary(tokenized_sentence, top_words=top_2000_words) -> dict[str, bool]:\n",
    "    word_occurrence_dictionary = to_word_occurrence_dictionary(tokenized_sentence, top_words)\n",
    "    word_presence_dictionary = {word: word_occurrence_dictionary[word] > 0 for word in word_occurrence_dictionary.keys()}\n",
    "    return word_presence_dictionary"
   ],
   "execution_count": 132,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 23906 unique words in the 'subjectivity' corpus\n",
      "The 2000 most common words are: {'escapes', 'stops', 'hollywood', 'ill', 'the', 'peace', 'friends', 'adult', 'possibly', 'single', 'intellectual', 'seemingly', 'somewhat', 'perhaps', 'kung', 'dealer', 'too', 'k', 'strong', 'goofy', 'victims', 'stone', 'ready', 'others', 'revolution', 'faced', 'ship', 'completely', 'turned', 'following', 'play', 'far', 'date', 'spend', 'running', 'lovers', 'about', 'sons', 'sean', 'pieces', 'appears', 'heart', 'finish', 'home', 'cell', 'seeking', 'band', 'emotionally', 'adults', 'seeks', 'beneath', 'then', 'account', 'expect', 'police', 'inner', 'suddenly', 'somewhere', 'beyond', 'force', 'entire', 'india', 'abandoned', 'enough', 'whose', 'leading', 'land', 'someone', 'turns', 'opportunity', 'touch', 'constructed', 'sets', 'y', 'returns', 'learn', 'jane', 'ultimate', 'rural', 'runs', 'dumb', 'ways', 'yourself', 'ugly', 'students', 'spectacle', 'points', 'otherwise', 'involved', '5', 'tedious', 'silly', 'musical', 'male', 'crew', 'desert', 'aftermath', 'highly', 'equivalent', 'step', 'place', 'accidentally', 'girl', 'sort', 'spirited', 'sam', 'husband', 'secrets', \"aren't\", 'storytelling', 'examination', 'feature', 'rarely', 'job', 'loss', 'intimate', '2001', 'arrested', 'discovered', 'conservative', 'personality', 'particular', 'slice', 'screenplay', 'colorful', 'foreign', 'across', 'forgettable', 'poetry', 'takes', 'stuff', 'possible', 'cinematic', 'apparent', 'thing', 'feels', 'matters', 'figure', 'money', 'cousin', 'poetic', 'you', 'succeeds', 'detective', 'time', 'captured', 'london', 'pain', 'mental', 'true', 'destiny', 'changes', 'breaks', 'night', 'b', 'chance', 'strike', '2', 'watch', 'got', 'fears', 'elements', 'average', 'hot', 'water', 'haunting', 'sustain', 'stuck', 'live', 'grows', 'advice', 'continue', 'keeping', 'peculiar', 'player', 'probably', 'terrific', 'by', 'result', 'while', 'working', '*', 'martial', 'showing', 'prison', 'surprisingly', 'unfortunately', 'weekend', 'always', 'jones', 'sent', 'original', 'culture', 'killers', 'national', 'word', 'jedi', 'here', '000', 'eyes', 'feel', 'part', 'better', 'decision', 'leaves', 'intentions', 'some', 'different', 'days', 'finds', 'now', 'desperation', 'paid', 'journalist', 'somehow', 'long', 'either', 'computer', 'slightly', 'party', 'bizarre', 'enjoyable', 'worthy', 'wild', 'calls', 'passion', 'girls', 'simon', 'bored', 'state', 'lazy', 'texas', 'excitement', 'show', 'makes', \"she's\", 'setting', 'strangely', 'huge', 'stories', 'surprise', 'board', 'francis', 'likely', 'towards', 'performances', 'sullivan', 'social', 'england', 'appeal', 'sick', 'acts', 'terrible', 'reporter', 'remember', 'genuinely', 'hero', 'white', 'concerned', 'standards', 'approach', 'youth', 'delivered', 'than', 'meditation', 'given', 'ability', 'suspects', 'joke', 'works', 'kids', 'features', 'intense', 'action', 'streets', 'sexy', 'quirky', 'became', 'sounds', 'shelter', 'sends', 'southern', 'ride', 'exciting', 'powers', 'memories', 'forces', 'trail', 'flicks', 'opera', 'arrival', 'never', 'assistant', 'niro', 'died', 'german', 'birthday', 'unusual', 'imagination', 'road', 'dies', 'dog', 'once', 'various', 'kidnap', 'she', 'asks', 'starts', 'capable', 'enter', 'powerful', 'charlie', 'arrive', 'effects', 'other', 'directing', 'deep', 'count', 'graham', 'innocent', \"women's\", 'chief', 'pulls', 'that', 'disturbing', 'imagery', 'perfect', 'merely', 'an', 'emotional', 'high', 'routine', 'adventures', 'mob', 'second', 'development', 'riveting', 'jason', 'believes', 'nick', 'contrived', 'except', 'though', 'win', 'department', 'theater', 'which', 'learns', 'memory', 'perfectly', 'common', 'role', 'american', 'bank', 'relationship', 'impact', 'close', 'mostly', 'slow', 'discovery', 'brought', 'escaped', 'worth', 'ago', 'felt', 'l', 'south', 'rock', 'actually', 'gun', 'mom', 'of', 'right', 'talking', \"mother's\", 'addition', 'investigate', 'old-fashioned', 'early', 'source', 'roger', 'cast', 'done', 'exactly', 'keith', 'herself', 'really', 'strange', 'viewers', 'your', 'dry', 'rachel', 'lawyer', 'survive', 'witness', 'decent', 'nothing', 'narrative', 'wish', 'mitchell', 'because', 'throws', 'crafted', 'physical', 'matter', 'seem', 'eddie', 'call', 'desperate', 'combat', \"they're\", 'feelings', 'sincere', 'exploration', 'themes', 'capture', 'sexual', 'says', 'roots', 'wanted', 'strikes', 'mountains', 'fashion', 'race', 'owner', 'screen', 'boyfriend', 'arts', 'fairy', 'las', 'target', 'couple', 'building', 'hunt', 'underworld', 'mitch', 'complex', 'straight', 'earth', 'anna', 'treasure', 'had', 'plans', 'court', 'ensemble', 'like', 'meets', 'stupid', 'message', 'this', '90', 'see', 'incredible', 'soundtrack', 'pull', 'north', 'add', 'form', 'means', 'red', 'balance', 'period', 'involving', 'existence', 'professional', 'power', 'men', 'circumstances', 'shooting', 'childhood', 'expected', 'facing', 'make', 'four', 'vegas', 'witty', 'bring', 'today', 'dogs', 'nearly', 'post', 'monsters', 'el', 'depth', 'suspense', 'filled', 'john', 'plot', 'certainly', 'equally', 'boring', 'drawn', 'paranoia', 'laughs', 'or', 'doing', 'best', 'fame', 'spent', 'fighting', 'agency', 'flawed', 'acting', 'mr', 'voice', 'actor', 'tim', 'fun', 'parker', 'barely', 'beauty', 'major', 'raised', 'together', 'seems', 'crisis', 'doctor', 'touching', 'in', 'throughout', 'led', 'impressive', 'guns', 'army', 'large', 'difficult', 'strength', 'encounters', 'innocence', 'wit', 'studio', 'luck', 'dramatic', 'dialogue', 'being', 'thoroughly', 'matt', 'those', 'gorgeous', 'ever', 'mike', 'just', 'popular', 'director', 'entertaining', 'named', 'comic', 'against', 'historical', 'president', 'basketball', 'acted', 'stunning', 'saw', 'cannot', 'fast', 'up', 'fear', 'open', 'spectacular', 'island', 'radio', 'interesting', '2002', 'kidnapped', 'sister', 'officer', 'vision', 'scenes', 'willing', 'compassion', 'deeply', 'places', 'visual', 'familiar', 'era', 'character', 'franchise', 'clear', 'funny', 'indeed', 'jewish', 'becoming', 'list', 'older', 'later', 'waiting', 'general', 'talk', 'field', 'years', 'challenge', 'changed', 'own', 'main', 'novel', 'pop', 'solve', 'loves', 'competition', 'gangster', 'yet', 'stage', 'guard', 'minute', 'pregnant', 'gradually', 'sweet', 'gripping', 'tribute', 'amusing', 'smart', 'can', 'attack', 'memorable', 'charm', 'leads', 'triangle', 'tape', 'apart', 'freddy', 'when', 'career', 'doubt', 'music', 'writer-director', 'loved', 'jack', 'exercise', 'depressing', 'their', 'max', 'concept', 'journey', 'heavy', 'alive', 'morning', 'only', 'mind', 'chilling', 'ended', 'eventually', 'give', 'driver', 'passes', 'situations', 'singer', 'travels', 'match', 'process', 'losing', 'worst', 'remake', 'cultural', 'fbi', 'flaws', 'cross', 'known', 'but', 'signs', 'seven', 'mood', 'kills', 'mountain', 'delicate', 'Â\\x96', 'realize', 'moving', 'confront', 'henry', 'realizes', 'teacher', 'turning', 'humor', 'adaptation', 'something', 'undercover', 'whole', 'several', 'convinced', 'adventure', 'camera', 'missed', 'block', 'behind', 'return', 'to', 'more', 'tried', 'sisters', 'her', 'stolen', 'buy', 'unexpected', 'they', 'california', 'stay', \"who's\", 'need', 'cut', 'farm', 'have', 'spiritual', 'provides', 'horror', 'young', 'determined', 'question', 'chaotic', 'roxie', 'weird', 'ghost', 'leaving', 'kyle', 'flat', 'nor', \"film's\", 'romantic', 'carlos', 'order', \"woman's\", 'former', 'else', 'debut', 'out', 'unknown', \"we've\", 'chris', 'may', 'films', 'slick', 'melodrama', 'room', 'test', 'eve', 'intelligence', 'troubled', 'little', 'using', 'editing', 'left', 'including', 'los', 'community', 'missing', 'successful', 'international', 'deadly', 'taken', '\"', 'surrounding', 'could', 'workers', 'xxx', 'lack', 'visually', 'filmmakers', 'manhattan', 'spell', 'investigation', 'mary', 'jokes', 'begins', 'food', 'say', 'interested', 'all', 'love', 'dangerous', 'level', 'coming', 'rap', 'tragedy', 'might', 'happiness', 'game', 'evidence', 'with', 'earlier', 'subjects', 'every', 'until', 'plain', 'baby', 'neighborhood', 'steven', \"couldn't\", 'film', 'become', 'imax', 'everyone', 'support', 'tension', 'generation', 'creature', 'create', 'back', 'inspired', 'deliver', 'nicholas', 'boy', 'drug', 'ambitious', 'quiet', 'joy', 'material', 'art', 'been', 'emerges', 'underground', 'east', 'will', 'did', 'for', 'hits', 'during', 'eye', 'full', 'fascinating', 'set', 'town', 'security', 'caught', 'pictures', 'act', 'holy', 'understand', 'hidden', 'cop', 'fly', 'violence', 'ben', 'number', 'exotic', 'obsession', 'delightful', 'violent', 'happens', 'magic', 'mess', 'nature', 'also', 'famous', 'cool', 'george', 'grand', 'manner', 'within', 'genuine', 'epic', 'months', 'thanks', 'legend', 'remote', 'subject', 'border', 'friendship', 'entirely', 'body', 'book', 'color', 'daniel', 'daily', 'without', 'inventive', 'handsome', 'amazing', \"you'd\", 'shame', 'guilt', \"he's\", 'are', 'ending', 'ii', 'shows', 'watching', 'un', 'he', 'murder', 'present', 'broken', 'guys', 'good', 'very', 'tired', 'follows', \"city's\", 'ms', 'picture', 'smoking', 'manages', 'whatever', 'stop', 'rob', 'fans', 'nuclear', 'committed', 'project', 'written', 'mix', 'personal', ':', 'cinema', 'human', 'stands', 'angry', 'itself', 'said', 'quest', 'questions', 'children', 'were', 'wrong', 'artist', 'jail', 'ted', 'van', 'hospital', 'such', 'rare', 'there', 'cia', 'usually', '&#38', 'experiences', 'age', 'quickly', 'chemistry', 'service', 'identity', 'finding', 'bad', 'series', 'recent', 'loses', 'turn', \"'the\", 'damage', 'value', 'released', 'is', 'agent', 'suffering', 'scientist', 'oscar', 'lonely', 'sports', 'bond', 'discover', 'daughter', 'premise', 'presents', 'event', 'shoes', 'drive', 'struggles', 'dance', 'weeks', 'used', 'hunter', 'gone', 'unable', 'simple', 'restaurant', 'facts', 'hip', 'solid', 'minutes', 'coming-of-age', 'dream', 'attention', 'sight', 'sign', 'partner', 'deal', 'efforts', 'information', 'story', 'planet', 'goes', 'cash', 'black', 'x', 'eccentric', 'chan', 'although', 'face', 'grace', 'elusive', ',', 'spends', 'talent', 'courage', 'near', 'way', 'year', 'surprises', 'rescue', 'space', 'holocaust', 'bland', 'side', 'truly', 'disney', 'direction', 'case', 'sees', 'certain', 'dr', 'definitely', 'greatest', 'college', 'school', 'fails', 'attitude', 'control', 'train', 'married', 'potential', 'script', 'house', 'comedic', 'supposed', 'contemporary', \"doesn't\", 'desire', 'on', 'constantly', 'real', 'beloved', 'chicago', 'stand', 'essentially', 'remarkable', 'neither', 'top', 'read', 'que', 'wonderful', 'twists', 'let', 'attempt', 'lines', 'soon', 'assigned', 'lover', 'minds', 'murdered', 'quite', 'despite', 'leave', 'private', 'helps', 'at', 'states', 'allows', 'lose', 'musicians', 'promises', 'con', '20', 'dying', 'psychological', 'bigger', 'reality', 'suspect', 'line', 'me', 'spring', 'usual', 'robbery', 'decided', 'remains', \"won't\", 'nice', 'having', 'unfolds', 'center', 'criminal', 'fellow', 'allen', 'come', 'pure', 'decide', 'god', \"isn't\", 'break', 'system', 'him', 'rather', 'lots', 'traditional', 'visit', 'miller', 'family', 'hands', 'differences', 'vampire', 'member', 'reading', 'growing', 'enters', 'interview', 'discovers', 'martin', 'twist', 'our', 'tell', 'dreams', 'searching', ';', 'fairly', 'creative', 'gets', 'example', 'animal', 'challenges', 'moves', 'living', \"didn't\", 'ancient', 'bride', 'normal', 'put', 'trying', 'reason', 'move', 'ideas', 'encounter', 'buried', 'america', 'executive', 'french', 'phone', 'playing', 'star', 'point', 'wait', 'finally', 'corrupt', 'artistic', 'less', 'indian', 'captain', 'since', 'light', 'tells', '(', 'evil', 'disaster', 'anyone', 'trip', 'miss', 'century', 'taste', 'wants', 'o', 'where', 'alien', 'outrageous', 'tale', 'woman', 'two', 'even', 'imagine', 'comes', 'gold', 'already', 'lost', 'situation', 'stars', 'small', 'pick', 'great', 'hell', 'none', 'occasionally', 'jim', 'nowhere', 'death', 'blood', 'future', 'trust', 'passed', 'media', 'world', 'name', 'boys', 'most', 'victim', 'british', 'born', 'protect', 'wife', 'million', 'creepy', 'success', 'lot', 'sharp', 'hit', 'soap', 'brings', 'delivers', 'seriously', 'fat', 'after', 'special', 'struggling', 'killer', 'roles', 'thriller', 'terms', 'lena', 'nightmare', 'himself', 'plan', 'actors', 'beautiful', 'odd', 'warm', 'writing', 'prostitute', 'overcome', 'no', 'damned', 'lets', 'loving', 'scott', 'whom', 'apartment', 'fire', 'johnny', 'gentle', 'digital', 'handle', 'necessary', 'work', 'as', 'everyday', 'pace', 'vincent', 'decades', 'women', 'each', 'larger', 'letter', 'quality', 'revolves', 'start', 'gift', 'started', 'politics', 'boss', 'title', 'sad', 'producer', 'ray', 'jackson', 'almost', 'government', 'sea', \"you've\", 'complicated', 'machine', 'captures', 'due', 'dealing', 'thrown', 'hour', 'chase', 'universal', 'fight', 'kind', 'dinner', 'making', 'care', 'happening', 'travel', 'city', 'religious', 'david', 'reach', 'washington', 'run', 'promising', 'happened', 'thomas', 'uses', '-', 'us', 'inside', 'crazy', 'marriage', 'television', 'however', 'style', 'demands', 'drugs', 'reveals', 'bears', 'surface', 'easily', 'affection', 'filmmaker', 'especially', 'found', 'extraordinary', 'thus', 'brutally', 'find', 'awkward', 'want', 'sensitive', \"i'm\", 'fine', 'would', 'thinking', 'events', 'front', 'news', 'consequences', 'enemy', 'obsessed', 'motion', 'u', 'brain', 'lesson', 'freedom', 'audiences', 'able', 'sex', 'try', 'head', 'fit', 'documentary', 'longer', 'trouble', 'crush', 'into', '!', 'sit', 'hopes', 'law', \"you're\", 'moral', 'biggest', 'painful', 'simply', 'zone', 'eric', 'walk', 'get', 'revenge', 'mafia', 'summer', 'closer', 'has', 'park', 'bed', 'bill', 'both', 'accessible', 'flick', 'romance', 'earnest', 'moved', 'assassin', 'history', 'williams', 'legendary', 'deeper', 'dragon', 'scary', 'una', 'between', 'person', 'outside', 'killed', \"what's\", 'lucy', 'same', 'considered', 'quick', 'plays', 'dark', 'york', 'deserves', 'heaven', 'lovely', 'his', 'past', 'parents', 'take', 'off', 'team', 'fan', 'war', 'not', 'begin', 'nonetheless', 'indie', 'scene', 'first', 'liked', 'marry', 'complete', 'anthony', 'levels', 'harry', 'shallow', 'onto', 'beautifully', 'toward', 'things', 'pretty', 'car', 'many', 'waste', 'female', 'families', 'humans', 'genre', 'political', 'christian', 'called', \"world's\", 'spy', 'santa', 'end', 'ten', 'guy', 'die', 'robert', 'it', 'comedies', 'free', 'forms', 'west', 'shot', 'issues', 'ring', 'fathers', 'dead', 'spirit', 'purpose', 'legacy', 'store', 'taking', 'william', 'grow', 'noble', 'going', 'seeing', 'rest', 'provide', 'games', 'perspective', 'week', 'becomes', 'serious', 'increasingly', \"movie's\", 'think', 'instead', 'faith', 'continues', 'bus', 'search', 'often', 'accident', 'survival', 'old', 'sure', 'billy', 'ends', 'help', 'above', 'fiction', 'document', 'sense', 'angel', \"'\", 'engrossing', 'performance', 's', '.', '10', 'explores', 'one', 'serial', 'seek', 'ice', 'took', 'among', 'la', 'catch', 'man', 'tragic', 'effect', 'change', 'public', 'choose', 'talented', 'slowly', 'urban', 'path', 'internet', 'professor', 'hide', 'and', 'mission', 'skin', 'sides', 'movie', 'battle', 'keep', 'intensity', 'steal', 'terribly', 'industry', 'drag', 'haunted', 'shop', 'russian', 'ahead', 'hoping', 'realistic', 'words', 'thoughtful', 'struggle', 'dad', 'moment', 'society', 'de', 'blind', 'from', 'hours', 'holds', 'ground', 'code', \"it's\", 'irish', 'use', 'formula', 'viewer', 'well', 'report', 'keeps', 'anything', 'images', 'falling', 'trapped', \"they've\", 'them', 'poignant', 'was', 'told', 'mexico', 'previous', 'course', 'class', 'further', 'convincing', 'suicide', 'happen', 'over', 'fall', 'experience', \"don't\", 'office', 'c', 'day', 'unsettling', 'getting', 'type', 'created', 'production', 'prevent', 'madness', 'answers', 'united', 'share', 'prove', 'effective', 'why', 'upon', 'blend', 'mediocre', 'decides', 'version', 'results', 'hong', 'follow', 'terrifying', 'understanding', 'mistake', 'excellent', 'camp', 'surprising', 'sitting', 'my', 'gives', 'execution', 'who', 'brutal', 'justice', 'twin', 'episode', 'homes', 'looks', 'frank', 'guilty', 'tv', 'look', 'around', 'bar', 'passionate', 'country', 'bit', 'business', 'master', 'bright', 'threat', 'desperately', 'j', 'suffers', 'key', 'jordan', 'track', 'before', 'lord', 'extremely', 'fully', 'tone', 'independent', 'plenty', 'leader', 'wealthy', \"year's\", 'lead', 'actions', 'needs', 'street', 'eight', 'jr', 'alone', '--', 'any', 'brilliant', 'gay', 'attorney', 'bear', 'bitter', 'respect', 'third', 'obvious', 'least', 'teenagers', 'village', 'looking', 'happy', 'kid', 'final', 'believe', 'forever', 'skills', 'sequel', 'we', 'melodramatic', 'hand', 'classic', 'lies', 'alex', 'hard', 'teen', 'intriguing', 'effort', 'should', 'carry', 'wonder', 'six', \"you'll\", 'fate', 'middle', 'creates', 'porn', 'disguise', 'grief', 'late', '11', 'what', 'based', 'under', 'be', 'patricia', 'if', 'company', 'thin', 'mother', 'hired', 'entertainment', 'so', 'made', 'played', 'score', 'unlike', 'drama', 'people', ')', 'filmmaking', 'extreme', 'claire', 'fish', 'promise', 'accept', 'rules', 'chinese', 'themselves', 'bobby', 'latest', 'escape', 'destroy', 'girlfriend', 'brothers', 'tales', '?', 'immediately', 'thought-provoking', 'know', 'younger', 'truth', 'skill', 'badly', 'hardly', 'literally', 'compelling', 'king', 'grant', 'down', 'responsible', 'rise', 'e', 'pretentious', 'save', 'research', 'ones', 'friend', 'last', 'monster', 'another', 'go', 'steve', 'honest', 'door', 'pleasure', 'everything', 'pursuit', 'landscape', 'satire', 'tough', 'japanese', \"father's\", 'killing', 'directed', 'short', 'sound', 'do', 'particularly', 'teams', 'tom', 'offer', 'falls', 'charming', 'modest', 'build', 'clever', 'james', 'few', 'problems', 'wall', 'animation', 'edge', 'times', 'thought', 'life', 'half', 'actress', 'amount', 'brother', 'how', 't', 'originality', 'parts', 'secret', 'fare', 'comedy', 'movies', 'adam', 'kill', 'piece', 'record', 'giving', 'san', 'thousands', 'michael', 'tries', 'christmas', 'lacks', 'worse', 'appear', 'interest', 'refuses', 'hold', \"there's\", 'murphy', 'gags', 'characters', 'winning', 'i', 'relationships', 'operation', 'interviews', 'consider', 'hilarious', 'values', 'came', 'met', 'crime', 'student', 'big', 'birth', 'away', 'woods', 'betrayal', 'price', 'surreal', 'cold', 'ultimately', 'seen', \"can't\", 'natural', \"family's\", 'sentimental', 'these', 'hair', 'portrait', 'soul', 'moments', 'faces', 'green', 'revealing', 'insight', 'lucky', 'central', 'does', 'length', 'audience', 'whether', 'problem', 'attempts', 'horrible', 'forget', 'pair', 'overall', 'dull', 'poor', 'task', 'queen', 'france', 'enjoy', 'affair', '30', 'nine', 'much', 'starting', 'again', 'utterly', 'poet', 'joe', 'sometimes', 'english', 'emotions', 'fresh', 'ordinary', 'footage', 'laugh', 'typical', 'wedding', 'tony', 'performers', 'appealing', 'paul', 'father', 'kong', 'proves', 'knows', 'air', 'must', 'viewing', 'teenage', 'trial', 'forced', 'twisted', 'offering', 'idea', 'site', 'members', 'peter', 'bruce', 'focus', 'danger', 'avoid', 'talks', 'unique', 'important', 'maybe', 'hope', 'military', 'notorious', 'carefully', 'local', 'offers', \"hasn't\", 'saved', 'modern', 'group', 'thinks', 'child', 'puts', 'danny', 'join', 'held', 'its', 'fantasy', 'mysterious', 'intelligent', 'easy', 'meanwhile', 'fact', 'impossible', 'view', 'video', 'engaging', 'yes', 'satisfying', 'knowing', 'sequences', 'beginning', 'gang', 'mark', 'veteran', \"man's\", 'provocative', 'absorbing', 'product', 'misses', 'blue', 'through', 'rich', 'energy', 'five', 'next', 'cute', 'meet', 'club', 'jenny', 'cause', 'along', 'subtle', 'still', 'enterprise', 'lee', 'humanity', 'son', 'new', 'unlikely', 'murders', 'pay', 'creating', 'clearly', 'angeles', 'soldiers', 'mexican', 'telling', 'study', 'arrives', 'lives', \"that's\", 'a', 'lifestyle', 'writer', 'mystery', 'detail', 'manager', 'conflict', 'predictable', 'meaning', 'feeling', 'three', 'heads'}\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vrxhu0iB99YN"
   },
   "source": [
    "## Question 3\n",
    "Complete the following tasks:\n",
    "1. Create a training set with 9,000 sentences (4,500 of each category)\n",
    "2. Create a test set with 1,000 sentences (500 of each category)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "08xWq_S899xK",
    "ExecuteTime": {
     "end_time": "2023-09-08T12:53:14.704205377Z",
     "start_time": "2023-09-08T12:52:50.321750406Z"
    }
   },
   "source": [
    "training_objective_set = [to_word_presence_dictionary(sentence) for sentence in category_sentences[0][:4500]]\n",
    "training_subjective_set = [to_word_presence_dictionary(sentence) for sentence in category_sentences[1][:4500]]\n",
    "training_set = [(sentence, 'obj') for sentence in training_objective_set] + [(sentence, 'subj') for sentence in training_subjective_set]\n",
    "\n",
    "testing_objective_set = [to_word_presence_dictionary(sentence) for sentence in category_sentences[0][4500:]]\n",
    "testing_subjective_set = [to_word_presence_dictionary(sentence) for sentence in category_sentences[1][4500:]]\n",
    "testing_set = [(sentence, 'obj') for sentence in testing_objective_set] + [(sentence, 'subj') for sentence in testing_subjective_set]"
   ],
   "execution_count": 133,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HToPuGzX9-Cl"
   },
   "source": [
    "## Question 4\n",
    "Complete the following tasks:\n",
    "1. Train a Naive Bayes classifier using the training set from the previous question.\n",
    "2. Evaluate the classifier on the test set. How accurate is it?\n",
    "3. Find the 20 most informative features."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "iHrN1tvKZa4e",
    "ExecuteTime": {
     "end_time": "2023-09-08T12:54:40.217035055Z",
     "start_time": "2023-09-08T12:53:14.724546618Z"
    }
   },
   "source": [
    "subjectivity_classifier = nltk.NaiveBayesClassifier.train(training_set)\n",
    "print(\"Train Accuracy:\", nltk.classify.accuracy(subjectivity_classifier, training_set))\n",
    "# Train Accuracy: 0.9198888888888889\n",
    "print(\"Accuracy:\", nltk.classify.accuracy(subjectivity_classifier, testing_set))\n",
    "# Accuracy: 0.906\n",
    "\n",
    "top_20_most_informative_features = subjectivity_classifier.most_informative_features(20)\n",
    "print(\"The 20 most informative features are: {}\".format(top_20_most_informative_features))\n"
   ],
   "execution_count": 134,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.9198888888888889\n",
      "Accuracy: 0.906\n",
      "The 20 most informative features are: [('--', True), ('order', True), ('decides', True), ('sister', True), ('entertaining', True), ('girlfriend', True), ('discover', True), (\"film's\", True), (\"you're\", True), ('daughter', True), ('married', True), ('amusing', True), ('plans', True), ('probably', True), ('plan', True), ('town', True), (\"you've\", True), ('kill', True), ('slow', True), ('interesting', True)]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Question 5\n",
    "Dialog acts are sort of the type of *action* performed by the speaker. In the instant messaging corpus dataset 'NPS', each utterance is labeled with one of 15 dialogue act types, such as **Statement**, **Emotion**, **ynQuestion**, **Continuer**, etc.\n",
    "\n",
    "Your task is to classify text from the NPS corpus into two dialog acts: **whQuestion** or **Emotion**.\n",
    "\n",
    "Start by downloading the NPS corpus and getting all posts from the corpus:"
   ],
   "metadata": {
    "id": "f-J9BEEQFrbo"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "nltk.download('nps_chat')\n",
    "posts = nltk.corpus.nps_chat.xml_posts()"
   ],
   "metadata": {
    "id": "Qtc_pG1qJZgh",
    "ExecuteTime": {
     "end_time": "2023-09-08T12:54:40.335367884Z",
     "start_time": "2023-09-08T12:54:40.218612301Z"
    }
   },
   "execution_count": 135,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package nps_chat to\n",
      "[nltk_data]     /home/administrator/nltk_data...\n",
      "[nltk_data]   Package nps_chat is already up-to-date!\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create a list that only includes posts of class **Emotion** and **whQuestion**. You can access the class of a post by calling `post.get(\"class\")`."
   ],
   "metadata": {
    "id": "p0wQn19UqfKV"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "emotion_posts = [post for post in posts if post.get(\"class\") == \"Emotion\"]\n",
    "wh_question_posts = [post for post in posts if post.get(\"class\") == \"whQuestion\"]\n",
    "print(\"There are {} posts of class 'Emotion' and {} posts of class 'whQuestion'\".format(len(emotion_posts), len(wh_question_posts)))\n",
    "# There are 1106 posts of class 'Emotion' and 533 posts of class 'whQuestion'\n",
    "# NOTE: the dataset is unbalanced, this may lead to classification errors\n",
    "\n",
    "classified_posts = {post: post.get(\"class\") for post in emotion_posts + wh_question_posts}\n",
    "classified_posts = {key: value for key, value in sorted(classified_posts.items(), key=lambda x: x[1])}\n",
    "print(\"The total amount of classified posts is {}\".format(len(classified_posts)))\n"
   ],
   "metadata": {
    "id": "H316uXLTrRUA",
    "ExecuteTime": {
     "end_time": "2023-09-08T12:54:41.815049520Z",
     "start_time": "2023-09-08T12:54:40.233699690Z"
    }
   },
   "execution_count": 136,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1106 posts of class 'Emotion' and 533 posts of class 'whQuestion'\n",
      "The total amount of classified posts is 1639\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Randomize the posts and create a training set and a test set, where the first 1300 **Emotion + whQuestion** posts are used for training and the rest for testing."
   ],
   "metadata": {
    "id": "t1K-jkXCzv4T"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import random\n",
    "\n",
    "random.seed(1234)\n",
    "# NOTE: seed add only to make results reproducible\n",
    "\n",
    "randomized_posts = list(classified_posts.items())\n",
    "random.shuffle(randomized_posts)\n",
    "\n",
    "training_set = randomized_posts[:1300]\n",
    "testing_set = randomized_posts[1300:]\n",
    "print(\"There are {} posts in the training set and {} posts in the test set\".format(len(training_set), len(testing_set)))\n",
    "# There are 1300 posts in the training set and 339 posts in the test set"
   ],
   "metadata": {
    "id": "1SQdx2rY0LG6",
    "ExecuteTime": {
     "end_time": "2023-09-08T12:54:41.975249373Z",
     "start_time": "2023-09-08T12:54:41.795534216Z"
    }
   },
   "execution_count": 137,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1300 posts in the training set and 339 posts in the test set\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create a list of the 200 most frequent tokens in the training set. You can access the text of a `post` object by calling `post.text`. Remember that the **split** function will use whitespace to tokenize a string: `some_string.split()`"
   ],
   "metadata": {
    "id": "38Klc5bHxDvW"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "all_tokens_in_train_set = []\n",
    "for post, _ in training_set:\n",
    "    all_tokens_in_train_set += nltk.word_tokenize(post.text)\n",
    "print(\"There are {} tokens in the training set\".format(len(all_tokens_in_train_set)))\n",
    "# There are 4944 tokens in the training set\n",
    "print(\"The first 5 tokens in the training set are: {}\".format(all_tokens_in_train_set[:5]))\n",
    "# The first 5 tokens in the training set are: ['what', 'happened', 'last', 'night', '?']\n",
    "\n",
    "frequency_distribution = nltk.FreqDist(all_tokens_in_train_set)\n",
    "top_200_tokens_posts = [token for token, _ in frequency_distribution.most_common(200)]\n",
    "print(\"The 200 most frequent tokens are: {}\".format(top_200_tokens_posts))"
   ],
   "metadata": {
    "id": "cmBZL8wxso-q",
    "ExecuteTime": {
     "end_time": "2023-09-08T12:54:42.128982044Z",
     "start_time": "2023-09-08T12:54:41.830589428Z"
    }
   },
   "execution_count": 138,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4944 tokens in the training set\n",
      "The first 5 tokens in the training set are: ['what', 'happened', 'last', 'night', '?']\n",
      "The 200 most frequent tokens are: [')', '?', '(', 'lol', '!', 'what', 'you', 'how', 'lmao', ':', 'who', 'are', 'the', 'is', 'to', ',', 'LOL', 'haha', 'u', 'why', 'where', 'and', 'whats', '.', '...', '-', ']', 'do', 'in', ';', 'up', 'did', \"'s\", 'a', 'so', '..', '[', 'it', 'here', 'omg', 'that', 'from', '....', 'me', 'about', 'for', 'was', 'What', 'I', 'damn', 'LMAO', 'of', 'everyone', '<', 'LoL', '11-09-40sUser18', 'r', 'we', '@', 'your', 'there', 'wants', 'i', 'on', 'ya', 'have', 'good', 'ha', '10-19-adultsUser23', 'all', '10-19-adultsUser35', 'asl', 'hey', 'been', 'like', '11-08-40sUser48', '11-08-adultsUser65', 'ok', 'when', 'hahah', '*', 'How', 'doing', '10-19-40sUser9', '10-24-40sUser16', 'Lol', 'today', 'oh', 'talk', '11-06-adultsUser105', 'they', '11-09-40sUser7', '11-09-40sUser48', '10-19-30sUser31', '11-09-40sUser30', '10-19-20sUser121', 'not', 'name', 'hows', '``', \"''\", 'yes', 'happened', '11-09-40sUser31', '10-19-40sUser34', 'doin', '11-09-40sUser39', '11-08-adultsUser3', 'wtf', '11-08-40sUser7', 'hugs', 'with', 'this', 'hehe', '10-24-40sUser34', 'tonight', 'her', 'my', 'huh', '11-08-20sUser21', 'Who', 'red', '10-19-30sUser17', \"n't\", 'hahaha', 'at', '10-19-adultsUser47', 'come', 'which', 'P', 'should', '10-19-adultsUser28', 'o', '10-19-30sUser11', 'round', 'too', 'but', 'Why', 'he', 'always', 'long', 'know', 'time', 'hi', 'just', '10-19-adultsUser32', 'fine', 'aw', '10-24-40sUser41', '10-19-30sUser9', 'im', '10-19-20sUser7', 'say', 'much', 'yay', 'sang', '11-09-40sUser49', 'girl', 'talkin', 'wow', '10-19-20sUser115', 'go', 'whos', 'many', '11-08-40sUser18', 'night', 'part', 'really', '11-08-40sUser66', 'now', 'be', 'o0', 'hehehehe', 'hehehe', 'well', '10-24-40sUser13', 'awww', '11-08-20sUser69', 'You', 'can', 'laffs', 'crap', '11-08-20sUser91', 'yall', 'pink', 'silver', 'Oh', 'yawn', 'am', 'cool', \"'\", '10-19-40sUser3', \"'d\", 'if', 'hell', '10-19-30sUser19', 'someone', 'girls', '10-19-30sUser25', 'ugh']\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "Define two feature selection functions that take a string as input and output a dictionary of features:\n",
    "* `get_word_features(string)`\n",
    "* `get_custom_features(string)`\n",
    "\n",
    "Begin by defining `get_word_features`. This function should use the words as features, just like in the movie review example above.\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "tyC-0es9KwTq"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def get_word_features(string):\n",
    "    tokens_from_string = nltk.word_tokenize(string)\n",
    "    return to_word_presence_dictionary(tokens_from_string, top_200_tokens_posts)"
   ],
   "metadata": {
    "id": "6b6mKRLiFqEo",
    "ExecuteTime": {
     "end_time": "2023-09-08T12:54:42.130370623Z",
     "start_time": "2023-09-08T12:54:42.030415794Z"
    }
   },
   "execution_count": 139,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, define `get_custom_features`. This function should extract the features from the text that characterize the **Emotion** and **whQuestions** classes."
   ],
   "metadata": {
    "id": "n4LmO-UaJCHj"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def get_custom_features(string):\n",
    "    custom_features = dict()\n",
    "    custom_features[\"contains_?\"] = \"?\" in string\n",
    "    custom_features[\"contains_!\"] = \"!\" in string\n",
    "    custom_features[\"post_length\"] = len(string) > 35\n",
    "    # NOTE: Threshold set manually, could be optimized\n",
    "    custom_features[\"contains_emoticon\"] = any([token in string for token in [\":)\", \":(\", \":D\", \":P\", \":/\", \":|\", \":O\", \":S\", \":*\", \":'(\"]])\n",
    "    custom_features[\"contains_laugh\"] = any([token in string for token in [\"haha\", \"hahaha\", \"lol\", \"lmao\", \"rofl\"]])\n",
    "    custom_features[\"contains_wh\"] = any([token in string for token in [\"what\", \"when\", \"where\", \"who\", \"why\", \"how\"]])\n",
    "    custom_features[\"contains_emotion\"] = any([token in string for token in [\"love\", \"hate\", \"like\", \"dislike\", \"sad\", \"happy\", \"angry\", \"mad\", \"annoyed\", \"excited\", \"bored\", \"scared\", \"fear\", \"afraid\", \"surprised\", \"surprise\", \"disgusted\", \"disgust\", \"shocked\", \"shock\", \"confused\", \"confuse\", \"confusing\", \"confusion\", \"depressed\", \"depress\", \"depressing\", \"depression\", \"anxious\", \"anxiety\", \"anxious\", \"anxiously\"]])\n",
    "    return custom_features"
   ],
   "metadata": {
    "id": "w9CBUttQKEzs",
    "ExecuteTime": {
     "end_time": "2023-09-08T12:54:42.131536158Z",
     "start_time": "2023-09-08T12:54:42.095971688Z"
    }
   },
   "execution_count": 140,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Conduct the following tasks:\n",
    "*   Train two Naive Bayes classifiers on the **Emotion + whQuestions** training set: one that uses the `get_word_features` function and another using `get_custom_features`.\n",
    "*   Evaluate each classifier on the test set. How accurate are they? Which one is better?\n",
    "*   What are the 20 most informative features for each classifier?\n"
   ],
   "metadata": {
    "id": "KxGa5GS1J3aG"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "bayes_classifier_training_set_word_features = [(get_word_features(post.text), post_class) for post, post_class in training_set]\n",
    "bayes_classifier_testing_set_word_features = [(get_word_features(post.text), post_class) for post, post_class in testing_set]\n",
    "\n",
    "word_features_naive_bayes_classifier = nltk.NaiveBayesClassifier.train(bayes_classifier_training_set_word_features)\n",
    "print(\"Train Accuracy:\", nltk.classify.accuracy(word_features_naive_bayes_classifier, bayes_classifier_training_set_word_features))\n",
    "print(\"Accuracy:\", nltk.classify.accuracy(word_features_naive_bayes_classifier, bayes_classifier_testing_set_word_features))\n",
    "print(\"The 20 most informative features are: {}\".format(word_features_naive_bayes_classifier.most_informative_features(20)))\n",
    "\n",
    "\n",
    "bayes_classifier_training_set_custom_features = [(get_custom_features(post.text), post_class) for post, post_class in training_set]\n",
    "bayes_classifier_testing_set_custom_features = [(get_custom_features(post.text), post_class) for post, post_class in testing_set]\n",
    "\n",
    "custom_features_naive_bayes_classifier = nltk.NaiveBayesClassifier.train(bayes_classifier_training_set_custom_features)\n",
    "print(\"Train Accuracy:\", nltk.classify.accuracy(custom_features_naive_bayes_classifier, bayes_classifier_training_set_custom_features))\n",
    "print(\"Accuracy:\", nltk.classify.accuracy(custom_features_naive_bayes_classifier, bayes_classifier_testing_set_custom_features))\n",
    "print(\"The 20 most informative features are: {}\".format(custom_features_naive_bayes_classifier.most_informative_features(20)))\n",
    "# NOTE: Lower train accuracy but higher test accuracy for custom features"
   ],
   "metadata": {
    "id": "15ERz1hnK8at",
    "ExecuteTime": {
     "end_time": "2023-09-08T12:54:43.418760573Z",
     "start_time": "2023-09-08T12:54:42.096422250Z"
    }
   },
   "execution_count": 141,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.9830769230769231\n",
      "Accuracy: 0.9852507374631269\n",
      "The 20 most informative features are: [('how', True), ('u', True), ('you', True), ('the', True), ('and', True), ('up', True), (\"'s\", True), ('so', True), ('lmao', True), ('from', True), ('me', True), ('to', True), ('lol', True), ('everyone', True), ('it', True), (')', True), ('(', True), ('hey', True), ('we', True), ('ok', True)]\n",
      "Train Accuracy: 0.9746153846153847\n",
      "Accuracy: 0.9882005899705014\n",
      "The 20 most informative features are: [('contains_wh', True), ('contains_laugh', True), ('contains_emoticon', True), ('contains_emotion', True), ('post_length', True), ('contains_wh', False), ('contains_?', False), ('contains_laugh', False), ('contains_!', True), ('post_length', False), ('contains_emoticon', False), ('contains_!', False), ('contains_emotion', False), ('contains_?', True)]\n"
     ]
    }
   ]
  }
 ]
}
