{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4gb8JZdRCj6L"
   },
   "source": [
    "# T-725 Natural Language Processing: Lab 3\n",
    "In today's lab, we will be working with logistic regression and part-of-speech tagging, and word embeddings.\n",
    "\n",
    "To begin with, do the following:\n",
    "* Select `\"File\" > \"Save a copy in Drive\"` to create a local copy of this notebook that you can edit.\n",
    "* Select `\"Runtime\" > \"Run all\"` to run the code in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rL4Pb6jbR0_j"
   },
   "source": [
    "## Extracting numerical features from text\n",
    "Machine learning algorithms generally only accept numerical input, meaning that we must represent all features numerically. For example, to classify a single sentence, we might pass a classifier a list of word counts in that sentence, or a list of `True` and `False` values (which have numerical values of 1 and 0, respectively), representing the presence or absence of particular words.\n",
    "\n",
    "[Scikit-learn](https://scikit-learn.org/stable/) is a popular machine learning library for Python that implements a wide variety of machine learning algorithms, including naive Bayesian and logistic regression. It also offers a convenient way to extract numerical features from text, for example with the [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) class. `CountVectorizer` is used to generates feature vectors containing character or word n-gram counts for any n within a given range (e.g., `ngram_range=(2, 2)` for only bigrams, or `ngram_range(1, 3)` for unigrams, bigrams and trigrams)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "1bMHZMO5TNbd",
    "ExecuteTime": {
     "end_time": "2023-09-15T12:14:28.551687953Z",
     "start_time": "2023-09-15T12:14:27.686267026Z"
    }
   },
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize a vectorizer that counts word bigrams\n",
    "vectorizer = CountVectorizer(ngram_range=(2, 2))\n",
    "\n",
    "# Count all bigrams in the sentences and create a feature vector\n",
    "sentences = [\"It was the best of times, it was the worst of times,\",\n",
    "             \"it was the age of wisdom, it was the age of foolishness,\"]\n",
    "\n",
    "vector = vectorizer.fit_transform(sentences)\n",
    "\n",
    "print(\"Bigrams:\", vectorizer.get_feature_names_out())\n",
    "print(\"\\nFeatures:\")\n",
    "print(vector.toarray())"
   ],
   "execution_count": 70,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigrams: ['age of' 'best of' 'it was' 'of foolishness' 'of times' 'of wisdom'\n",
      " 'the age' 'the best' 'the worst' 'times it' 'was the' 'wisdom it'\n",
      " 'worst of']\n",
      "\n",
      "Features:\n",
      "[[0 1 2 0 2 0 0 1 1 1 2 0 1]\n",
      " [2 0 2 1 0 1 2 0 0 0 2 1 0]]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ie-9LuaLX_Yw"
   },
   "source": [
    "Here, `vectorizer` created a matrix with 13 columns (one for each bigram) and two rows (one for each sentence). Each row consists of bigram counts for the corresponding sentence. For example, the first sentence has the bigram counts `[0 1 2 0 2 0 0 1 1 1 2 0 1]`, which means that it contains 0 instances of \"age of\", 1 instance of \"best of\", two instances of \"it was\", and so on (we can see which column represents which bigram with `vecorizer.get_feature_names()`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y1Jb9cBxhTMh"
   },
   "source": [
    "## Creating training and test sets\n",
    "Scikit-learn lets us quickly split data into training and test sets with the [train_test_split()](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) function. Note that by convention, examples are generally denoted with a capital X while labels are denoted with a lowercase y. Let's create a training and test set for the subjectivity corpus from the NLTK:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "4YbXr6-KhYDR",
    "ExecuteTime": {
     "end_time": "2023-09-15T12:14:28.763512062Z",
     "start_time": "2023-09-15T12:14:27.710435806Z"
    }
   },
   "source": [
    "import nltk\n",
    "from nltk.corpus import subjectivity\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Download the subjectivity corpus and get the sentences for each category\n",
    "nltk.download('subjectivity')\n",
    "\n",
    "obj_fileids = subjectivity.fileids('obj')\n",
    "subj_fileids = subjectivity.fileids('subj')\n",
    "\n",
    "# Let's get the untokenized sentences from each category\n",
    "obj_sentences = subjectivity.raw(obj_fileids).splitlines()\n",
    "subj_sentences = subjectivity.raw(subj_fileids).splitlines()\n",
    "\n",
    "X = obj_sentences + subj_sentences\n",
    "y = ['obj'] * 5000 + ['subj'] * 5000\n",
    "\n",
    "# Create a word unigram count vectorizer and generate the feature vectors\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 1))\n",
    "X_vectorized = vectorizer.fit_transform(X)\n",
    "\n",
    "# Create a training and test set (80%/20% split). This function always shuffles\n",
    "# the examples before making the split, but we can make sure that it always\n",
    "# shuffles them the same way by specifying a specific random_state value.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_vectorized,\n",
    "                                                    y,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=42)"
   ],
   "execution_count": 71,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package subjectivity to\n",
      "[nltk_data]     /home/administrator/nltk_data...\n",
      "[nltk_data]   Package subjectivity is already up-to-date!\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wCTmsvXzgY-C"
   },
   "source": [
    "## Logistic regression\n",
    "We can create a logistic regression classifier with the [LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) class:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "TGffjsuFghoi",
    "ExecuteTime": {
     "end_time": "2023-09-15T12:14:28.906758272Z",
     "start_time": "2023-09-15T12:14:27.892913309Z"
    }
   },
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "subj_clf = LogisticRegression(solver='liblinear')\n",
    "subj_clf.fit(X_train, y_train)  # Train the model\n",
    "score = subj_clf.score(X_test, y_test)  # Evaluate the model on the test set\n",
    "print(\"Accuracy: {:.1%}\".format(score))"
   ],
   "execution_count": 72,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 90.2%\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ZEznKZAuj1-"
   },
   "source": [
    "Our logistic regression classifier obtains an accuracy of 90.2%, which is quite a bit higher than the accuracy obtained by NLTK's naive Bayes classifier in a previous lab.\n",
    "\n",
    "Once the classifier is trained, we can use it to classify new sentences:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "3Ev9cbxGunWk",
    "ExecuteTime": {
     "end_time": "2023-09-15T12:14:28.908295269Z",
     "start_time": "2023-09-15T12:14:28.196271607Z"
    }
   },
   "source": [
    "example_sentences = [\n",
    "  \"Monty Python's Flying Circus, the British comedy group which gained fame via\\\n",
    "   BBC-TV, send-up Arthurian legend, performed in whimsical fashion with Graham\\\n",
    "   Chapman an effective straight man as King Arthur.\",\n",
    "  \"The funniest movie of 1975 and probably the silliest movie ever made.\"\n",
    "]\n",
    "\n",
    "features = vectorizer.transform(example_sentences)\n",
    "subj_clf.predict(features)"
   ],
   "execution_count": 73,
   "outputs": [
    {
     "data": {
      "text/plain": "array(['obj', 'subj'], dtype='<U4')"
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JKiN7bhwfk0j"
   },
   "source": [
    "## Pipelines\n",
    "Instead of having to call `vectorizer.transform()` every time we use the classifier, we can create a `Pipeline` that automatically extracts features for us."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "nMuh7usegvTw",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "outputId": "e422df05-cd6c-409a-8c28-1587b5254245",
    "ExecuteTime": {
     "end_time": "2023-09-15T12:14:29.057869467Z",
     "start_time": "2023-09-15T12:14:28.200836479Z"
    }
   },
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer(ngram_range=(1, 1))),\n",
    "    ('clf', LogisticRegression(solver='liblinear'))\n",
    "])\n",
    "\n",
    "# The feature vectors are automatically created\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=42)\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "score = pipeline.score(X_test, y_test)\n",
    "print(\"Accuracy: {:.1%}\".format(score))\n",
    "\n",
    "pipeline.predict(example_sentences)"
   ],
   "execution_count": 74,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 90.2%\n"
     ]
    },
    {
     "data": {
      "text/plain": "array(['obj', 'subj'], dtype='<U4')"
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c8AqtdrsdfZ0"
   },
   "source": [
    "## Creating word embeddings\n",
    "[Gensim](https://radimrehurek.com/gensim/) is a Python library that makes it easy to generate and work with word embeddings.\n",
    "\n",
    "Let's start by supressing some warnings from Gensim:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "R7ElxTOtl6UQ",
    "ExecuteTime": {
     "end_time": "2023-09-15T12:14:29.058365867Z",
     "start_time": "2023-09-15T12:14:28.661754706Z"
    }
   },
   "source": [
    "import warnings\n",
    "\n",
    "# Suppress some warnings from Gensim about deprecated functions\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=DeprecationWarning)"
   ],
   "execution_count": 75,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, let's create word2vec embeddings for NLTK's movie review corpus:"
   ],
   "metadata": {
    "id": "fs583ebKoiXK"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "hLvQMJDrdvdW",
    "ExecuteTime": {
     "end_time": "2023-09-15T12:14:35.508209668Z",
     "start_time": "2023-09-15T12:14:28.688383408Z"
    }
   },
   "source": [
    "import nltk\n",
    "from nltk.corpus import movie_reviews\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "nltk.download('movie_reviews')\n",
    "nltk.download('punkt')\n",
    "\n",
    "sents = movie_reviews.sents()\n",
    "movie_embeddings = Word2Vec(sents, epochs=1, min_count=5, vector_size=50)"
   ],
   "execution_count": 76,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     /home/administrator/nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/administrator/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iHF4ReN-DVr3"
   },
   "source": [
    "What does the vector for *actor* look like?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "DpOAPLLktUmz",
    "ExecuteTime": {
     "end_time": "2023-09-15T12:14:35.532979103Z",
     "start_time": "2023-09-15T12:14:35.515890460Z"
    }
   },
   "source": [
    "movie_embeddings.wv['actor']"
   ],
   "execution_count": 77,
   "outputs": [
    {
     "data": {
      "text/plain": "array([-0.14465444,  0.13068557, -0.17789738, -0.06775393, -0.34196997,\n       -0.35266912,  1.0072047 ,  0.6979135 , -1.1438864 , -0.38752666,\n       -0.07723561, -0.68953025,  0.5680722 ,  0.4822872 , -0.3422061 ,\n        0.393016  ,  0.5331087 ,  0.19988902, -1.1739159 , -0.7145756 ,\n        0.26494   ,  0.709854  ,  0.9475496 , -0.19953738,  0.46094087,\n        0.3408349 ,  0.05129399, -0.05970282, -0.55898684,  0.06849612,\n       -0.09603619, -0.69361496,  0.13180181, -0.21880607, -0.30816236,\n        0.23028538,  0.5024911 ,  0.03674503,  0.15560886, -0.34253106,\n        0.5585809 , -0.4490201 ,  0.42188516,  0.26859817,  1.4237837 ,\n       -0.0027669 , -0.13546555, -0.730442  ,  0.5303456 ,  0.36504996],\n      dtype=float32)"
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LEfsQ7TJCutn"
   },
   "source": [
    "# Assignment\n",
    "Answer the following questions and hand in your solution in Canvas before 8:30 AM, Monday September 18th. Remember to save your file before uploading it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a6Zayf9rC2cP"
   },
   "source": [
    "## Question 1\n",
    "The NLTK includes a copy of the *Universal Declaration of Human Rights* (UDHR) in over 300 languages, including Icelandic, Norwegian, Swedish, Danish, Finnish and Faroese.\n",
    "\n",
    "Create a `Pipeline` with a `CountVectorizer` and a `LogisticRegression` classifier that satisfies the following requirements:\n",
    "\n",
    "The `CountVectorizer` should:\n",
    "* Create character-level n-grams.\n",
    "* Generate unigram, bigram and trigram counts.\n",
    "\n",
    "The `LogisticRegression` classifier should:\n",
    "* Use the `liblinear` solver.\n",
    "\n",
    "Refer to Scikit-learn's reference for the [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) and [LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) for information on possible parameters.\n",
    "\n",
    "Once you've created the pipeline, train it using the `train_udhr(pipeline)` function below, which returns the test examples and labels (and should not be modified). Report the accuracy of the classifier, and try making predictions on a few sentences from these languages, for example from Wikipedia ([is](https://is.wikipedia.org/wiki/Fors%C3%AD%C3%B0a), [no](https://no.wikipedia.org/wiki/Portal:Forside), [se](https://sv.wikipedia.org/wiki/Portal:Huvudsida), [da](https://da.wikipedia.org/wiki/Forside), [fi](https://fi.wikipedia.org/wiki/Wikipedia:Etusivu), [fo](https://fo.wikipedia.org/wiki/Fors%C3%AD%C3%B0a)). One sentence from each language is enough. Does the classifier perform as well as you would expect, given the reported accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Gypk375K55Zj",
    "ExecuteTime": {
     "end_time": "2023-09-15T12:14:35.600020007Z",
     "start_time": "2023-09-15T12:14:35.541381297Z"
    }
   },
   "source": [
    "# Don't change anything in this code cell\n",
    "import random\n",
    "from nltk.corpus import udhr\n",
    "nltk.download('udhr')\n",
    "\n",
    "def train_udhr(pipeline):\n",
    "  X = []\n",
    "  y = []\n",
    "\n",
    "  # The UDHR is quite small, so let's create 1,000 \"fake\" sentences in each\n",
    "  # language by randomly stringing together 3-15 words.\n",
    "  for lang in languages:\n",
    "    words = udhr.words(lang)\n",
    "    sents = [\" \".join(random.choices(words, k=random.randint(3, 15))) for x in range(1000)]\n",
    "    X.extend(sents)\n",
    "    y += [lang] * len(sents)\n",
    "\n",
    "  X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                      y,\n",
    "                                                      test_size=0.1,\n",
    "                                                      random_state=42)\n",
    "\n",
    "  # Train the classifier\n",
    "  pipeline.fit(X_train, y_train)\n",
    "  return X_test, y_test\n",
    "\n",
    "languages = ['Icelandic_Yslenska-Latin1',\n",
    "             'Norwegian-Latin1',\n",
    "             'Swedish_Svenska-Latin1',\n",
    "             'Danish_Dansk-Latin1',\n",
    "             'Finnish_Suomi-Latin1',\n",
    "             'Faroese-Latin1']"
   ],
   "execution_count": 78,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package udhr to\n",
      "[nltk_data]     /home/administrator/nltk_data...\n",
      "[nltk_data]   Package udhr is already up-to-date!\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "i-sM02NcDrZb",
    "ExecuteTime": {
     "end_time": "2023-09-15T12:14:35.881832677Z",
     "start_time": "2023-09-15T12:14:35.580629361Z"
    }
   },
   "source": [
    "pipe = Pipeline([\n",
    "    ('vect', CountVectorizer(ngram_range=(1, 3), analyzer='char')),\n",
    "    ('clf', LogisticRegression(solver='liblinear'))\n",
    "])\n",
    "\n",
    "test_X, test_y = train_udhr(pipe)\n",
    "\n",
    "results = pipe.score(test_X, test_y)\n",
    "print(\"Test Accuracy: \", results)\n",
    "\n",
    "is_sentence_test = \"Enginn aðgangseyrir var að viðburðum leikanna og sóttu Parísarbúar og allra þjóða hermenn þá í stríðum straumum. Áætlað er að heildarfjöldi áhorfenda hafi verið nærri hálf milljón manna. Keppendur voru um 1.500 talsins, Bandaríkjamenn flestir þeirra.\"\n",
    "no_sentence_test = \"Deep Purple er eit hardrockband som vart danna i 1968 i Hertford i England. Gruppa har ved sida av Black Sabbath og Led Zeppelin vorte rekna som pionerar innan hardrock og heavy metal, sjølv om somme bandmedlemmar hevdar at ein ikkje kan kategorisere musikken deira til ein enkelt sjanger.\"\n",
    "sv_sentence_test = \"Sveriges utrikes- och säkerhetspolitik under kalla kriget utformades efter andra världskrigets slut med hänsyn till landets geografiska position mellan de två militärallianserna Nato och Warszawapakten; perioden täcker åren från 1945 till 1989.\"\n",
    "da_sentence_test = \"The Miracles er en amerikansk rhythm and blues-gruppe fra Detroit. The Miracles var den første gruppe på pladeselskabet Motown Records, der opnåede større succes. The Miracles' \\\"Shop Around\\\" blev Motowns første millionsælgende hitsingle, og gruppen identificerede i høj grad Motown op gennem 1960'erne.\"\n",
    "fi_sentence_test = \"Vihermittari (Thalera fimbrialis) on vihermittareihin kuuluva keskikokoinen vihreä perhoslaji, jota tavataan laajalla alueella Euroopassa ja Aasiassa. Suomessa vihermittari on harvinainen etelärannikon laji, joka on havaittu maan nykyisten rajojen sisäpuolelta ensimmäisen kerran vuonna 1955.\"\n",
    "fo_sentence_test = \"Hiram King Williams, Sr., betri kendur sum Hank Williams (føddur 17. september 1923 í Alabama, deyður 1. januar 1953 í West Virginia) var ein amerikanskur countrysangari og sangskrivari. Hann verður mettur at vera ein av teimum týdningarmiklastu og ein av teimum ið hava havt størstu ávirkanina á aðrir tónleikarar í 20. øld\"\n",
    "\n",
    "predict_result = pipe.predict([is_sentence_test, no_sentence_test, sv_sentence_test, da_sentence_test, fi_sentence_test, fo_sentence_test])\n",
    "predict_and_expected = list(zip(predict_result, languages))\n",
    "predict_and_expected_str = \"\\n\".join([f\"Predicted: {p[0]}, Expected: {p[1]}\" for p in predict_and_expected])\n",
    "print(f\"\\nPredicted and expected languages:\\n{predict_and_expected_str}\")"
   ],
   "execution_count": 79,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  0.9633333333333334\n",
      "\n",
      "Predicted and expected languages:\n",
      "Predicted: Icelandic_Yslenska-Latin1, Expected: Icelandic_Yslenska-Latin1\n",
      "Predicted: Norwegian-Latin1, Expected: Norwegian-Latin1\n",
      "Predicted: Swedish_Svenska-Latin1, Expected: Swedish_Svenska-Latin1\n",
      "Predicted: Danish_Dansk-Latin1, Expected: Danish_Dansk-Latin1\n",
      "Predicted: Finnish_Suomi-Latin1, Expected: Finnish_Suomi-Latin1\n",
      "Predicted: Faroese-Latin1, Expected: Faroese-Latin1\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HwetpCxTCwXB"
   },
   "source": [
    "## Question 2\n",
    "The logistic regression classifier below tries to determine which of the following tags should be assigned to a given word:\n",
    "* **NP** (proper nouns, singular),\n",
    "* **NP\\$** (proper nouns, singular and possessive),\n",
    "* **VBG** (verbs, present participle) or\n",
    "* **VBD** (verbs, past tense).\n",
    "\n",
    "The classifier makes its determination solely on characteristics of the word itself and does not make use of any contextual features. The function `extract_features(word)` extracts a list of numerical features from each word, currently only the length of a word and whether or not it ends with \"r\". Using these features, the classifier obtains an accuracy of 37.1%, which is quite poor. Replace the features that the `exctract_features()` function generates with your own. Use Python's [string methods](https://docs.python.org/3/library/stdtypes.html#string-methods) to generate the features, and try to get at least 99% accuracy.\n",
    "\n",
    "**Remember**: each feature must be numerical (or `True`/`False`), and don't forget to add a comma after each feature in the list."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "-be0psqueELC",
    "ExecuteTime": {
     "end_time": "2023-09-15T12:14:36.660970752Z",
     "start_time": "2023-09-15T12:14:35.899323249Z"
    }
   },
   "source": [
    "# Don't change anything in this code cell\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import brown\n",
    "\n",
    "nltk.download('brown')\n",
    "\n",
    "def get_brown_tags(tag):\n",
    "  return sorted({w for s in brown_train for w, t in s if t == tag})\n",
    "\n",
    "def train_model():\n",
    "  # Create the training set\n",
    "  word_list = [word for tag_words in words for word in tag_words]\n",
    "  X = [extract_features(word) for word in word_list]\n",
    "  y = [tag for tag, tag_words in zip(tags, words) for word in tag_words]\n",
    "\n",
    "  # Train and evaluate the classifier\n",
    "  log_clf = LogisticRegression(solver='liblinear', multi_class='ovr')\n",
    "  log_clf.fit(X, y)\n",
    "  print(\"Accuracy: {:.1%}\".format(log_clf.score(X, y)))\n",
    "\n",
    "  # Print the accuracy for each tag\n",
    "  predictions = log_clf.predict(X)\n",
    "  errors = defaultdict(list)\n",
    "  for word, example, label, prediction in zip(word_list, X, y, predictions):\n",
    "    if label != prediction:\n",
    "      errors[label].append(word)\n",
    "\n",
    "  print(\"\\nAccuracy and first 10 errors per tag:\")\n",
    "  for tag, tag_words in zip(tags, words):\n",
    "    error_words = errors[tag]\n",
    "    num_total = len(tag_words)\n",
    "    num_correct = num_total - len(error_words)\n",
    "    ratio = num_correct / num_total\n",
    "    print(\"{:>3} {:,}/{:,} ({:.1%}) {}\".format(tag, num_correct, num_total, ratio,\n",
    "                                              \", \".join(error_words[:10])))\n",
    "\n",
    "# Download and prepare the Brown corpus for training and testing\n",
    "brown_train, brown_test = train_test_split(brown.tagged_sents(),\n",
    "                                           test_size=0.1,\n",
    "                                           random_state=42)\n",
    "\n",
    "print(\"Training sentences: {:,}\".format(len(brown_train)))\n",
    "print(\"Test sentences: {:,}\".format(len(brown_test)))\n",
    "\n",
    "# Get 1,000 examples of each tag\n",
    "tags = ['NP', 'NP$', 'VBG', 'VBD']\n",
    "random.seed(42)\n",
    "words = [random.sample(get_brown_tags(tag), 1000) for tag in tags]"
   ],
   "execution_count": 80,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     /home/administrator/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training sentences: 51,606\n",
      "Test sentences: 5,734\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Ujpyyq8KGjN9",
    "ExecuteTime": {
     "end_time": "2023-09-15T12:14:36.805855977Z",
     "start_time": "2023-09-15T12:14:36.689688472Z"
    }
   },
   "source": [
    "# Modify the features generated by this function and run the code cell to see\n",
    "# how your changes affect the accuracy of the classifier.\n",
    "def extract_features(word):\n",
    "    # regex was faster...\n",
    "    features = [\n",
    "        word.istitle(),\n",
    "        any(char.isupper() for char in word),\n",
    "        word.isupper(),\n",
    "        \n",
    "        word.endswith('\\'s'),\n",
    "        word.endswith('\\''),\n",
    "        \n",
    "        word.endswith('ing'),\n",
    "        word.endswith('in'),\n",
    "        word.endswith('in\\''),\n",
    "        \n",
    "        word.endswith('ed'),\n",
    "        word.endswith('ed'),\n",
    "        word.endswith('ed'),\n",
    "    ]\n",
    "\n",
    "    return features\n",
    "\n",
    "# The errors listed by this function are words belonging to that tag that were\n",
    "# incorrectly assigned with another tag. Use them to figure out useful features.\n",
    "train_model()"
   ],
   "execution_count": 81,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 99.4%\n",
      "\n",
      "Accuracy and first 10 errors per tag:\n",
      " NP 983/1,000 (98.3%) Whiting, 400-401, Wilfred, niger, Diffring, aerogenes, Fred, anhemolyticus, Ring, Kooning\n",
      "NP$ 999/1,000 (99.9%) Grevyles\n",
      "VBG 999/1,000 (99.9%) waitin\n",
      "VBD 994/1,000 (99.4%) Sat, Came, Became, Ran, Thought, Got\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EiH2WoVmCyAV"
   },
   "source": [
    "## Question 3\n",
    "Word embeddings can capture semantic and syntactic relationships between words. For example, the vector between the words *king* and *man* is identical to the vector between *queen* and *woman* (i.e., *king* is to *man* as *queen* is to *woman*). This means that if we have a good vector representation for each of those words, we should be able to apply vector arithmetic to find that *king* - *man* + *woman* = *queen*.\n",
    "\n",
    "The function `find_word(a, b, x)`, defined below, finds the word **y**, such that **a** is to **b** as **x** is to **y** (also expressed as **a**:**b** as **x**:**y**).\n",
    "\n",
    "Below, we download GloVe word vectors through Gensim's API. Use those vectors and `find_words()` to complete the following tasks:\n",
    "1. In the UK, people say *petrol* instead of *gas*. Find the British English equivalent of the word *truck*.\n",
    "2. Find the capital of France.\n",
    "3. Find the present tense of the verb *flew*.\n",
    "\n",
    "**Note**: all words in `glove-wiki-gigaword-100` are in lowercase!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "jRFy2beIqXUa",
    "ExecuteTime": {
     "end_time": "2023-09-15T12:14:37.529580549Z",
     "start_time": "2023-09-15T12:14:36.754298522Z"
    }
   },
   "source": [
    "import gensim.downloader as api\n",
    "glove = api.load(\"glove-wiki-gigaword-100\")\n",
    "\n",
    "def find_word(a, b, x):\n",
    "  # a is to b as x is to ?\n",
    "  a = a.lower()\n",
    "  b = b.lower()\n",
    "  x = x.lower()\n",
    "  print(f\"> {a}:{b} as {x}:?\")\n",
    "  top_words = glove.most_similar_cosmul(positive=[x, b], negative=[a])\n",
    "  for num, (word, score) in enumerate(top_words[:5]):\n",
    "    print(f\"{num + 1}: ({score:.3f}) {word}\")\n",
    "  print()"
   ],
   "execution_count": 82,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "2iqF6z-8okux",
    "ExecuteTime": {
     "end_time": "2023-09-15T12:14:38.154096289Z",
     "start_time": "2023-09-15T12:14:37.535108036Z"
    }
   },
   "source": [
    "# Example 1: man is to king as woman is to ?\n",
    "find_word('man', 'king', 'woman')\n",
    "\n",
    "# Example 2: evening is to dinner as noon is to ?\n",
    "find_word('evening', 'dinner', 'noon')\n",
    "\n",
    "# 1) In the UK, people say 'petrol' instead of 'gas'. Find the British English\n",
    "find_word('gas', 'truck', 'petrol')\n",
    "\n",
    "\n",
    "# 2) Find the capital of France. Remember to use only lowercase characters.\n",
    "find_word('italy', 'rome', 'france')\n",
    "\n",
    "# 3) Find the present tense of the verb \"flew\".\n",
    "find_word('ate', 'eat', 'flew')\n"
   ],
   "execution_count": 83,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> man:king as woman:?\n",
      "1: (0.896) queen\n",
      "2: (0.850) monarch\n",
      "3: (0.845) throne\n",
      "4: (0.837) princess\n",
      "5: (0.836) elizabeth\n",
      "\n",
      "> evening:dinner as noon:?\n",
      "1: (0.839) lunch\n",
      "2: (0.829) breakfast\n",
      "3: (0.814) a.m.\n",
      "4: (0.814) p.m.\n",
      "5: (0.813) meal\n",
      "\n",
      "> gas:truck as petrol:?\n",
      "1: (1.020) lorry\n",
      "2: (0.957) wagon\n",
      "3: (0.951) trucks\n",
      "4: (0.950) lorries\n",
      "5: (0.945) car\n",
      "\n",
      "> italy:rome as france:?\n",
      "1: (0.995) paris\n",
      "2: (0.882) prohertrib\n",
      "3: (0.867) strasbourg\n",
      "4: (0.861) brussels\n",
      "5: (0.854) london\n",
      "\n",
      "> ate:eat as flew:?\n",
      "1: (1.025) fly\n",
      "2: (0.940) flying\n",
      "3: (0.897) flight\n",
      "4: (0.894) flown\n",
      "5: (0.893) flights\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3-phpyFFCyjL"
   },
   "source": [
    "## Question 4\n",
    "Gensim offers us several ways to find words that are similar or dissimilar to one another. Complete the following tasks:\n",
    "1. Use `glove.most_similar(word, topn=5)` to find the five words that are most similar to:\n",
    "  1. cat\n",
    "  2. samsung\n",
    "  3. batman\n",
    "2. Use `glove.doesnt_match(list_of_strings)` to find which of the words below doesn't fit with the rest:\n",
    "  1. cat hamster gremlin rabbit goldfish dog\n",
    "  2. samsung microsoft dell panasonic mcdonalds facebook\n",
    "  3. batman spiderman daredevil shrek hulk deadpool"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "rYGVE8bHCzA6",
    "ExecuteTime": {
     "end_time": "2023-09-15T12:14:38.288244338Z",
     "start_time": "2023-09-15T12:14:38.137926937Z"
    }
   },
   "source": [
    "most_similar_to_cat = glove.most_similar('cat', topn=5)\n",
    "print(most_similar_to_cat)\n",
    "most_similar_to_samsung = glove.most_similar('samsung', topn=5)\n",
    "print(most_similar_to_samsung)\n",
    "most_similar_to_batman = glove.most_similar('batman', topn=5)\n",
    "print(most_similar_to_batman)\n",
    "doesnt_match_gremline = glove.doesnt_match(['cat', 'hamster', 'gremlin', 'rabbit', 'goldfish', 'dog'])\n",
    "print(doesnt_match_gremline)\n",
    "doesnt_match_mcdonalds = glove.doesnt_match(['samsung', 'microsoft', 'dell', 'panasonic', 'mcdonalds', 'facebook'])\n",
    "print(doesnt_match_mcdonalds)\n",
    "doesnt_match_shrek = glove.doesnt_match(['batman', 'spiderman', 'daredevil', 'shrek', 'hulk', 'deadpool'])\n",
    "print(doesnt_match_shrek)"
   ],
   "execution_count": 84,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('dog', 0.8798074722290039), ('rabbit', 0.7424426674842834), ('cats', 0.732300341129303), ('monkey', 0.7288709878921509), ('pet', 0.7190139889717102)]\n",
      "[('lg', 0.8194020986557007), ('toshiba', 0.7769338488578796), ('hyundai', 0.7322311997413635), ('fujitsu', 0.7246403694152832), ('panasonic', 0.7154008746147156)]\n",
      "[('superman', 0.8058774471282959), ('superhero', 0.6820072531700134), ('sequel', 0.6592289209365845), ('catwoman', 0.6541578769683838), ('joker', 0.6362104415893555)]\n",
      "gremlin\n",
      "mcdonalds\n",
      "shrek\n"
     ]
    }
   ]
  }
 ]
}
